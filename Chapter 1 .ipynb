{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffdd1b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"D:/NLP/NLP-a-day-keeps-doctors-away\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50dc7848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cf37f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"IMDB_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e65af1fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I grew up (b. 1965) watching and loving the Th...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I put this movie in my DVD player, and sa...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why do people who do not know what a particula...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Even though I have great interest in Biblical ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Im a die hard Dads Army fan and nothing will e...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  I grew up (b. 1965) watching and loving the Th...  negative\n",
       "1  When I put this movie in my DVD player, and sa...  negative\n",
       "2  Why do people who do not know what a particula...  negative\n",
       "3  Even though I have great interest in Biblical ...  negative\n",
       "4  Im a die hard Dads Army fan and nothing will e...  positive"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cc7eb8",
   "metadata": {},
   "source": [
    "1. Tokenization\n",
    "\n",
    "Tokenization is the process of breaking down text into smaller units, typically words or phrases. It's a fundamental step in text preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eea2e177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'grew', 'up', '(', 'b', '.', '1965', ')', 'watching', 'and', 'loving', 'the', 'Thunderbirds', '.', 'All', 'my', 'mates', 'at', 'school', 'watched', '.', 'We', 'played', '``', 'Thunderbirds', \"''\", 'before', 'school', ',', 'during', 'lunch', 'and', 'after', 'school', '.', 'We', 'all', 'wanted', 'to', 'be', 'Virgil', 'or', 'Scott', '.', 'No', 'one', 'wanted', 'to', 'be', 'Alan', '.', 'Counting', 'down', 'from', '5', 'became', 'an', 'art', 'form', '.', 'I', 'took', 'my', 'children', 'to', 'see', 'the', 'movie', 'hoping', 'they', 'would', 'get', 'a', 'glimpse', 'of', 'what', 'I', 'loved', 'as', 'a', 'child', '.', 'How', 'bitterly', 'disappointing', '.', 'The', 'only', 'high', 'point', 'was', 'the', 'snappy', 'theme', 'tune', '.', 'Not', 'that', 'it', 'could', 'compare', 'with', 'the', 'original', 'score', 'of', 'the', 'Thunderbirds', '.', 'Thankfully', 'early', 'Saturday', 'mornings', 'one', 'television', 'channel', 'still', 'plays', 'reruns', 'of', 'the', 'series', 'Gerry', 'Anderson', 'and', 'his', 'wife', 'created', '.', 'Jonatha', 'Frakes', 'should', 'hand', 'in', 'his', 'directors', 'chair', ',', 'his', 'version', 'was', 'completely', 'hopeless', '.', 'A', 'waste', 'of', 'film', '.', 'Utter', 'rubbish', '.', 'A', 'CGI', 'remake', 'may', 'be', 'acceptable', 'but', 'replacing', 'marionettes', 'with', 'Homo', 'sapiens', 'subsp', '.', 'sapiens', 'was', 'a', 'huge', 'error', 'of', 'judgment', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\taaha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Example text\n",
    "example_text = df['review'][0]\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(example_text)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aea34b",
   "metadata": {},
   "source": [
    "2. Stemming\n",
    "\n",
    "Stemming is the process of reducing words to their word stem or root form. For instance, “fishing”, “fished”, “fisher” all reduce to the stem “fish”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fb6a74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'grew', 'up', '(', 'b', '.', '1965', ')', 'watch', 'and', 'love', 'the', 'thunderbird', '.', 'all', 'my', 'mate', 'at', 'school', 'watch', '.', 'we', 'play', '``', 'thunderbird', \"''\", 'befor', 'school', ',', 'dure', 'lunch', 'and', 'after', 'school', '.', 'we', 'all', 'want', 'to', 'be', 'virgil', 'or', 'scott', '.', 'no', 'one', 'want', 'to', 'be', 'alan', '.', 'count', 'down', 'from', '5', 'becam', 'an', 'art', 'form', '.', 'i', 'took', 'my', 'children', 'to', 'see', 'the', 'movi', 'hope', 'they', 'would', 'get', 'a', 'glimps', 'of', 'what', 'i', 'love', 'as', 'a', 'child', '.', 'how', 'bitterli', 'disappoint', '.', 'the', 'onli', 'high', 'point', 'wa', 'the', 'snappi', 'theme', 'tune', '.', 'not', 'that', 'it', 'could', 'compar', 'with', 'the', 'origin', 'score', 'of', 'the', 'thunderbird', '.', 'thank', 'earli', 'saturday', 'morn', 'one', 'televis', 'channel', 'still', 'play', 'rerun', 'of', 'the', 'seri', 'gerri', 'anderson', 'and', 'hi', 'wife', 'creat', '.', 'jonatha', 'frake', 'should', 'hand', 'in', 'hi', 'director', 'chair', ',', 'hi', 'version', 'wa', 'complet', 'hopeless', '.', 'a', 'wast', 'of', 'film', '.', 'utter', 'rubbish', '.', 'a', 'cgi', 'remak', 'may', 'be', 'accept', 'but', 'replac', 'marionett', 'with', 'homo', 'sapien', 'subsp', '.', 'sapien', 'wa', 'a', 'huge', 'error', 'of', 'judgment', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Initialize the Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Stemming\n",
    "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "print(stemmed_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0344eec4",
   "metadata": {},
   "source": [
    "3. Lemmatization\n",
    "\n",
    "Lemmatization is similar to stemming but it brings context to the words. It links words with similar meanings to one word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e10c3810",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\taaha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'grew', 'up', '(', 'b', '.', '1965', ')', 'watching', 'and', 'loving', 'the', 'Thunderbirds', '.', 'All', 'my', 'mate', 'at', 'school', 'watched', '.', 'We', 'played', '``', 'Thunderbirds', \"''\", 'before', 'school', ',', 'during', 'lunch', 'and', 'after', 'school', '.', 'We', 'all', 'wanted', 'to', 'be', 'Virgil', 'or', 'Scott', '.', 'No', 'one', 'wanted', 'to', 'be', 'Alan', '.', 'Counting', 'down', 'from', '5', 'became', 'an', 'art', 'form', '.', 'I', 'took', 'my', 'child', 'to', 'see', 'the', 'movie', 'hoping', 'they', 'would', 'get', 'a', 'glimpse', 'of', 'what', 'I', 'loved', 'a', 'a', 'child', '.', 'How', 'bitterly', 'disappointing', '.', 'The', 'only', 'high', 'point', 'wa', 'the', 'snappy', 'theme', 'tune', '.', 'Not', 'that', 'it', 'could', 'compare', 'with', 'the', 'original', 'score', 'of', 'the', 'Thunderbirds', '.', 'Thankfully', 'early', 'Saturday', 'morning', 'one', 'television', 'channel', 'still', 'play', 'rerun', 'of', 'the', 'series', 'Gerry', 'Anderson', 'and', 'his', 'wife', 'created', '.', 'Jonatha', 'Frakes', 'should', 'hand', 'in', 'his', 'director', 'chair', ',', 'his', 'version', 'wa', 'completely', 'hopeless', '.', 'A', 'waste', 'of', 'film', '.', 'Utter', 'rubbish', '.', 'A', 'CGI', 'remake', 'may', 'be', 'acceptable', 'but', 'replacing', 'marionette', 'with', 'Homo', 'sapiens', 'subsp', '.', 'sapiens', 'wa', 'a', 'huge', 'error', 'of', 'judgment', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize the Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatization\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "print(lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7bdae8",
   "metadata": {},
   "source": [
    "4. Bag of Words\n",
    "\n",
    "The Bag of Words (BoW) model converts text into a numerical representation where each document is represented by a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad46812b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\taaha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\taaha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# Preprocess a subset of reviews\n",
    "preprocessed_reviews = df['review'].head().apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a216df45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    grew b 1965 watching loving thunderbirds mates...\n",
       "1    put movie dvd player sat coke chips expectatio...\n",
       "2    people know particular time past like feel nee...\n",
       "3    even though great interest biblical movies bor...\n",
       "4    im die hard dads army fan nothing ever change ...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0572cf0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 1 1]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the preprocessed reviews\n",
    "bow_matrix = vectorizer.fit_transform(preprocessed_reviews)\n",
    "\n",
    "# Display the BoW matrix\n",
    "print(bow_matrix.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b64eac6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of BoW matrix: (5, 348)\n",
      "Some words in vocabulary: ['1965' '1969' 'abraham' 'abuse' 'accents' 'acceptable' 'acting' 'actual'\n",
      " 'adventurous' 'afternoon']\n"
     ]
    }
   ],
   "source": [
    "# Get the feature names (vocabulary)\n",
    "words = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Display the shape of the matrix and some words from the vocabulary\n",
    "print(\"Shape of BoW matrix:\", bow_matrix.shape)\n",
    "print(\"Some words in vocabulary:\", words[:10])  # Display the first 10 words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d625df7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
